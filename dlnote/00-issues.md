# 应当在每次训练中打乱训练数据

torch.utils.data.DataLoader(mnist_test, batch_size, **shuffle=False**, num_workers=get_dataloader_workers()))

> 训练数据在每个 epoch 中都被打乱，增加模型训练的多样性和有效性。

# mlp中隐藏层的大小必须要比输入层小吗

> 隐藏层的大小并不必须比输入层小。隐藏层的大小可以是任意的。

## 隐藏层大小选择的考虑因素

- 模型表达能力: 较大的隐藏层可以增加模型的表达能力，使其能够学习到更复杂的特征和模式。

- 计算成本: 较大的隐藏层会增加计算成本，可能会增加训练时间和内存使用。

- 过拟合风险: 较大的隐藏层可能导致模型记住训练数据，而不是学习其特征。这是因为模型有更多的参数可以用来拟合训练数据的噪声。因此，适当的正则化（如L2正则化、Dropout等）是必要的，以防止过拟合。

# 前向传播与反向传播的概念

## 前向传播

前向传播是神经网络计算输出的过程。给定输入数据，前向传播将数据通过网络中的各层，逐步计算每一层的输出，最终得到网络的预测结果。

## 反向传播

反向传播是神经网络训练中用于计算梯度并更新模型参数的过程。它基于链式法则，计算损失函数关于每个模型参数的梯度，从而调整参数以最小化损失函数。
